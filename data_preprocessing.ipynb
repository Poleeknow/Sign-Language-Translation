{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.711991Z","iopub.status.busy":"2023-10-27T11:37:06.711618Z","iopub.status.idle":"2023-10-27T11:37:06.717134Z","shell.execute_reply":"2023-10-27T11:37:06.715909Z","shell.execute_reply.started":"2023-10-27T11:37:06.711961Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import torch\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.722792Z","iopub.status.busy":"2023-10-27T11:37:06.722436Z","iopub.status.idle":"2023-10-27T11:37:06.731403Z","shell.execute_reply":"2023-10-27T11:37:06.730573Z","shell.execute_reply.started":"2023-10-27T11:37:06.722755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU: NVIDIA GeForce GTX 1650 Ti\n"]}],"source":["# Check if CUDA is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Using CPU\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.733450Z","iopub.status.busy":"2023-10-27T11:37:06.733137Z","iopub.status.idle":"2023-10-27T11:37:06.744000Z","shell.execute_reply":"2023-10-27T11:37:06.743207Z","shell.execute_reply.started":"2023-10-27T11:37:06.733423Z"},"trusted":true},"outputs":[],"source":["# Example of transferring a tensor to GPU\n","tensor_on_gpu = torch.Tensor([1.0, 2.0]).to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.745449Z","iopub.status.busy":"2023-10-27T11:37:06.745114Z","iopub.status.idle":"2023-10-27T11:37:06.813346Z","shell.execute_reply":"2023-10-27T11:37:06.812422Z","shell.execute_reply.started":"2023-10-27T11:37:06.745416Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>attachment_id</th>\n","      <th>text</th>\n","      <th>user_id</th>\n","      <th>height</th>\n","      <th>width</th>\n","      <th>length</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>44e8d2a0-7e01-450b-90b0-beb7400d2c1e</td>\n","      <td>Ё</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>76.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>df5b08f0-41d1-4572-889c-8b893e71069b</td>\n","      <td>А</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>40.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>17f53df4-c467-4aff-9f48-20687b63d49a</td>\n","      <td>Р</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>57.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>e3add916-c708-4339-ad98-7e2740be29e9</td>\n","      <td>Е</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>64.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bd7272ed-1850-48f1-a2a8-c8fed523dc37</td>\n","      <td>Ч</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>84.0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          attachment_id text  \\\n","0  44e8d2a0-7e01-450b-90b0-beb7400d2c1e    Ё   \n","1  df5b08f0-41d1-4572-889c-8b893e71069b    А   \n","2  17f53df4-c467-4aff-9f48-20687b63d49a    Р   \n","3  e3add916-c708-4339-ad98-7e2740be29e9    Е   \n","4  bd7272ed-1850-48f1-a2a8-c8fed523dc37    Ч   \n","\n","                            user_id  height  width  length  train  \n","0  185bd3a81d9d618518d10abebf0d17a8    1920   1080    76.0   True  \n","1  185bd3a81d9d618518d10abebf0d17a8    1920   1080    40.0   True  \n","2  185bd3a81d9d618518d10abebf0d17a8    1920   1080    57.0   True  \n","3  185bd3a81d9d618518d10abebf0d17a8    1920   1080    64.0   True  \n","4  185bd3a81d9d618518d10abebf0d17a8    1920   1080    84.0   True  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["annot = pd.read_csv(\"data/slovo/annotations.csv\", sep='\\t')\n","annot.head(5)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.815653Z","iopub.status.busy":"2023-10-27T11:37:06.815352Z","iopub.status.idle":"2023-10-27T11:37:06.822726Z","shell.execute_reply":"2023-10-27T11:37:06.821728Z","shell.execute_reply.started":"2023-10-27T11:37:06.815628Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['много', 'стоять', 'адаптивное поведение', 'переваривать',\n","       'расслабление', 'аккуратный', 'обучать', 'расписание', 'отчаянный',\n","       'наружу'], dtype=object)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["labels = annot['text'].sample(10)\n","labels.values"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.824691Z","iopub.status.busy":"2023-10-27T11:37:06.824039Z","iopub.status.idle":"2023-10-27T11:37:06.852914Z","shell.execute_reply":"2023-10-27T11:37:06.852004Z","shell.execute_reply.started":"2023-10-27T11:37:06.824656Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>attachment_id</th>\n","      <th>text</th>\n","      <th>user_id</th>\n","      <th>height</th>\n","      <th>width</th>\n","      <th>length</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1870</th>\n","      <td>2590430a-cddf-460c-83dc-5a9b75f5a836</td>\n","      <td>аккуратный</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>39.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1886</th>\n","      <td>d988c0b8-8418-47e3-8f07-89a109ff2023</td>\n","      <td>адаптивное поведение</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>54.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1989</th>\n","      <td>507d6f3c-f2b9-4411-8b0d-e6c4ef168725</td>\n","      <td>аккуратный</td>\n","      <td>0211b488644476dd0fec656ccb9b74fc</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>23.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2013</th>\n","      <td>f7693961-c80f-4e38-afc9-5c32ea34c479</td>\n","      <td>аккуратный</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>45.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2027</th>\n","      <td>e44625da-e950-41aa-91b2-d3303576d7eb</td>\n","      <td>аккуратный</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>25.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14608</th>\n","      <td>a6c18197-dbf4-4d2f-a941-99800cc3ca57</td>\n","      <td>расписание</td>\n","      <td>b07a773bcb10b4f14f33d2b0e8ec58ba</td>\n","      <td>720</td>\n","      <td>1280</td>\n","      <td>75.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14657</th>\n","      <td>275c7816-5d3b-4396-a084-e1b24a466d6b</td>\n","      <td>расписание</td>\n","      <td>46dd04a1caa75ed3082b573cb5a3ad26</td>\n","      <td>1920</td>\n","      <td>822</td>\n","      <td>55.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14712</th>\n","      <td>c9837056-17a2-4e53-9883-feca2ca26203</td>\n","      <td>расписание</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>59.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14800</th>\n","      <td>0ceb887d-ee4b-4826-8074-135c1f6ff399</td>\n","      <td>расписание</td>\n","      <td>95af8e702c909eee7145c6dc1a3d756b</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>57.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14925</th>\n","      <td>c8fe44c9-eb2a-49bc-898a-941dc61f7c5a</td>\n","      <td>расписание</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>59.0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 7 columns</p>\n","</div>"],"text/plain":["                              attachment_id                  text  \\\n","1870   2590430a-cddf-460c-83dc-5a9b75f5a836            аккуратный   \n","1886   d988c0b8-8418-47e3-8f07-89a109ff2023  адаптивное поведение   \n","1989   507d6f3c-f2b9-4411-8b0d-e6c4ef168725            аккуратный   \n","2013   f7693961-c80f-4e38-afc9-5c32ea34c479            аккуратный   \n","2027   e44625da-e950-41aa-91b2-d3303576d7eb            аккуратный   \n","...                                     ...                   ...   \n","14608  a6c18197-dbf4-4d2f-a941-99800cc3ca57            расписание   \n","14657  275c7816-5d3b-4396-a084-e1b24a466d6b            расписание   \n","14712  c9837056-17a2-4e53-9883-feca2ca26203            расписание   \n","14800  0ceb887d-ee4b-4826-8074-135c1f6ff399            расписание   \n","14925  c8fe44c9-eb2a-49bc-898a-941dc61f7c5a            расписание   \n","\n","                                user_id  height  width  length  train  \n","1870   db573f94204e56e0cf3fc2ea000e5bdc    1280    720    39.0   True  \n","1886   db573f94204e56e0cf3fc2ea000e5bdc    1280    720    54.0   True  \n","1989   0211b488644476dd0fec656ccb9b74fc    1920   1080    23.0   True  \n","2013   db573f94204e56e0cf3fc2ea000e5bdc    1280    720    45.0   True  \n","2027   185bd3a81d9d618518d10abebf0d17a8    1920   1080    25.0   True  \n","...                                 ...     ...    ...     ...    ...  \n","14608  b07a773bcb10b4f14f33d2b0e8ec58ba     720   1280    75.0   True  \n","14657  46dd04a1caa75ed3082b573cb5a3ad26    1920    822    55.0   True  \n","14712  db573f94204e56e0cf3fc2ea000e5bdc    1280    720    59.0   True  \n","14800  95af8e702c909eee7145c6dc1a3d756b    1280    720    57.0   True  \n","14925  185bd3a81d9d618518d10abebf0d17a8    1920   1080    59.0   True  \n","\n","[150 rows x 7 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["train150 = annot.query(\"text in @labels and train\")\n","train150"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.854822Z","iopub.status.busy":"2023-10-27T11:37:06.854405Z","iopub.status.idle":"2023-10-27T11:37:06.865720Z","shell.execute_reply":"2023-10-27T11:37:06.864720Z","shell.execute_reply.started":"2023-10-27T11:37:06.854791Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(50, 7)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["val50 = annot.query(\"text in @labels and not train\")\n","val50.shape"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.876002Z","iopub.status.busy":"2023-10-27T11:37:06.875714Z","iopub.status.idle":"2023-10-27T11:37:06.892510Z","shell.execute_reply":"2023-10-27T11:37:06.891825Z","shell.execute_reply.started":"2023-10-27T11:37:06.875977Z"},"trusted":true},"outputs":[],"source":["def crop_frame(frame):\n","    \"\"\"\n","    Crops the frame to a square shape\n","    :param frame: frame to crop\n","    :return: cropped frame\n","    \"\"\"\n","    height, width = frame.shape[:2]\n","    th_dim = frame.shape[2]\n","    max_dim = max(height, width)\n","    dif = abs(height-width)\n","\n","    first_side = dif // 2\n","    second_side = dif - first_side\n","    \n","    \n","    if width == max_dim:\n","        f_array = np.zeros(shape=(first_side, max_dim, th_dim))\n","        s_array = np.zeros(shape=(second_side, max_dim, th_dim))\n","        frame = np.concatenate((f_array, np.array(frame), s_array), axis=0)\n","    else:\n","        f_array = np.zeros(shape=(max_dim, first_side, th_dim))\n","        s_array = np.zeros(shape=(max_dim, second_side, th_dim))\n","        frame = np.concatenate((f_array, np.array(frame), s_array), axis=1)\n","\n","    return frame"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.896094Z","iopub.status.busy":"2023-10-27T11:37:06.895781Z","iopub.status.idle":"2023-10-27T11:37:06.905974Z","shell.execute_reply":"2023-10-27T11:37:06.905087Z","shell.execute_reply.started":"2023-10-27T11:37:06.896069Z"},"trusted":true},"outputs":[],"source":["def load_video(path, img_size):\n","    \"\"\"\n","    Loads the video from the path and returns a list of frames\n","    :param path: path to the video\n","    :param img_size: size of the image\n","    :param i: index of the video\n","    :return: list of frames\n","    \"\"\"\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        else:\n","            frame = crop_frame(frame)\n","            frame = cv2.resize(frame, (img_size, img_size))\n","            frame = frame[:, :, [2, 1, 0]]\n","            frame_tensor = torch.Tensor(frame).permute(2, 0, 1).to(device)\n","            frames.append(frame_tensor)\n","    return frames"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.918405Z","iopub.status.busy":"2023-10-27T11:37:06.918022Z","iopub.status.idle":"2023-10-27T11:37:06.929814Z","shell.execute_reply":"2023-10-27T11:37:06.928992Z","shell.execute_reply.started":"2023-10-27T11:37:06.918371Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","tensor_dir = \"data/tensors\"\n","Path(tensor_dir).mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.931233Z","iopub.status.busy":"2023-10-27T11:37:06.930915Z","iopub.status.idle":"2023-10-27T11:37:06.946282Z","shell.execute_reply":"2023-10-27T11:37:06.945513Z","shell.execute_reply.started":"2023-10-27T11:37:06.931205Z"},"trusted":true},"outputs":[],"source":["def process_and_save_tensors(annot_subset, subset_name, subdir_name = \"train\"):\n","    \"\"\"\n","    Processes and saves tensors to disk\n","    :param annot_subset: DataFrame with annotations\n","    :param subset_name: name of the subset\n","    :return: None\n","    \"\"\"\n","    i = 0\n","    for ind, row in annot_subset.iterrows():\n","        path = row['attachment_id']\n","        full_path = \"data/slovo/\" + str(subdir_name) + \"/\" + str(path) + \".mp4\"\n","\n","        # Load and process video\n","        frames = load_video(full_path, 100)\n","    \n","        # Save tensor to disk and store path in DataFrame\n","        tensor_dir = \"data/tensors\"\n","        tensor_path = os.path.join(tensor_dir, f\"{subset_name}_{path}.pt\")\n","        torch.save(frames, tensor_path)\n","        annot_subset.loc[ind, 'attachment_id'] = str(tensor_path)\n","        \n","        i += 1\n","        if i <= 150 and i % 10 == 0:\n","            print(f\"We are done on the image number {i}\")\n","    \n","    # Save DataFrame to CSV\n","    annot_subset.to_csv(f\"data/processed_annotations_{subset_name}.csv\", index=False)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:37:06.948215Z","iopub.status.busy":"2023-10-27T11:37:06.947326Z","iopub.status.idle":"2023-10-27T11:41:47.490152Z","shell.execute_reply":"2023-10-27T11:41:47.489193Z","shell.execute_reply.started":"2023-10-27T11:37:06.948190Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["We are done on the image number 10\n","We are done on the image number 20\n","We are done on the image number 30\n","We are done on the image number 40\n","We are done on the image number 50\n","We are done on the image number 60\n","We are done on the image number 70\n","We are done on the image number 80\n","We are done on the image number 90\n","We are done on the image number 100\n","We are done on the image number 110\n","We are done on the image number 120\n","We are done on the image number 130\n","We are done on the image number 140\n","We are done on the image number 150\n"]}],"source":["# Process and save tensors for training and validation subsets\n","process_and_save_tensors(train150, \"train\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:41:47.491694Z","iopub.status.busy":"2023-10-27T11:41:47.491370Z","iopub.status.idle":"2023-10-27T11:41:47.508673Z","shell.execute_reply":"2023-10-27T11:41:47.507821Z","shell.execute_reply.started":"2023-10-27T11:41:47.491666Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>attachment_id</th>\n","      <th>text</th>\n","      <th>user_id</th>\n","      <th>height</th>\n","      <th>width</th>\n","      <th>length</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>data/tensors\\train_2590430a-cddf-460c-83dc-5a9...</td>\n","      <td>аккуратный</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>39.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>data/tensors\\train_d988c0b8-8418-47e3-8f07-89a...</td>\n","      <td>адаптивное поведение</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>54.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>data/tensors\\train_507d6f3c-f2b9-4411-8b0d-e6c...</td>\n","      <td>аккуратный</td>\n","      <td>0211b488644476dd0fec656ccb9b74fc</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>23.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>data/tensors\\train_f7693961-c80f-4e38-afc9-5c3...</td>\n","      <td>аккуратный</td>\n","      <td>db573f94204e56e0cf3fc2ea000e5bdc</td>\n","      <td>1280</td>\n","      <td>720</td>\n","      <td>45.0</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>data/tensors\\train_e44625da-e950-41aa-91b2-d33...</td>\n","      <td>аккуратный</td>\n","      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n","      <td>1920</td>\n","      <td>1080</td>\n","      <td>25.0</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       attachment_id                  text  \\\n","0  data/tensors\\train_2590430a-cddf-460c-83dc-5a9...            аккуратный   \n","1  data/tensors\\train_d988c0b8-8418-47e3-8f07-89a...  адаптивное поведение   \n","2  data/tensors\\train_507d6f3c-f2b9-4411-8b0d-e6c...            аккуратный   \n","3  data/tensors\\train_f7693961-c80f-4e38-afc9-5c3...            аккуратный   \n","4  data/tensors\\train_e44625da-e950-41aa-91b2-d33...            аккуратный   \n","\n","                            user_id  height  width  length  train  \n","0  db573f94204e56e0cf3fc2ea000e5bdc    1280    720    39.0   True  \n","1  db573f94204e56e0cf3fc2ea000e5bdc    1280    720    54.0   True  \n","2  0211b488644476dd0fec656ccb9b74fc    1920   1080    23.0   True  \n","3  db573f94204e56e0cf3fc2ea000e5bdc    1280    720    45.0   True  \n","4  185bd3a81d9d618518d10abebf0d17a8    1920   1080    25.0   True  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["proc_annot = pd.read_csv(\"data/processed_annotations_train.csv\")\n","proc_annot.head()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:41:47.510929Z","iopub.status.busy":"2023-10-27T11:41:47.510217Z","iopub.status.idle":"2023-10-27T11:41:47.542031Z","shell.execute_reply":"2023-10-27T11:41:47.541167Z","shell.execute_reply.started":"2023-10-27T11:41:47.510896Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["We are done on the image number 10\n","We are done on the image number 20\n","We are done on the image number 30\n","We are done on the image number 40\n","We are done on the image number 50\n"]}],"source":["process_and_save_tensors(val50, \"valid\", \"test\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:43:34.711394Z","iopub.status.busy":"2023-10-27T11:43:34.710996Z","iopub.status.idle":"2023-10-27T11:43:34.738631Z","shell.execute_reply":"2023-10-27T11:43:34.737637Z","shell.execute_reply.started":"2023-10-27T11:43:34.711362Z"},"trusted":true},"outputs":[],"source":["# Load training annotations\n","annot_train150 = pd.read_csv(\"data/processed_annotations_train.csv\")\n","\n","# Load the first tensor\n","first_train_tensor_path = annot_train150['attachment_id'].iloc[0]\n","first_train_tensor = torch.load(first_train_tensor_path)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:43:39.452411Z","iopub.status.busy":"2023-10-27T11:43:39.451612Z","iopub.status.idle":"2023-10-27T11:43:39.459366Z","shell.execute_reply":"2023-10-27T11:43:39.458316Z","shell.execute_reply.started":"2023-10-27T11:43:39.452351Z"},"trusted":true},"outputs":[{"data":{"text/plain":["39"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(first_train_tensor)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:43:48.881408Z","iopub.status.busy":"2023-10-27T11:43:48.881015Z","iopub.status.idle":"2023-10-27T11:43:48.905656Z","shell.execute_reply":"2023-10-27T11:43:48.904553Z","shell.execute_reply.started":"2023-10-27T11:43:48.881374Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The first frame is not NaN or zeros.\n"]}],"source":["if torch.isnan(first_train_tensor[0]).any():\n","    print(\"The first frame contains NaN values!\")\n","elif torch.equal(first_train_tensor[0], torch.zeros_like(first_train_tensor[0])):\n","    print(\"The first frame is all zeros!\")\n","else:\n","    print(\"The first frame is not NaN or zeros.\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-27T11:44:18.283011Z","iopub.status.busy":"2023-10-27T11:44:18.282651Z","iopub.status.idle":"2023-10-27T11:44:18.290649Z","shell.execute_reply":"2023-10-27T11:44:18.289705Z","shell.execute_reply.started":"2023-10-27T11:44:18.282983Z"},"trusted":true},"outputs":[],"source":["# Load validation annotations\n","annot_val50 = pd.read_csv(\"data/processed_annotations_valid.csv\")\n","\n","# Load the first tensor\n","first_val_tensor_path = annot_train150['attachment_id'].iloc[0]\n","first_val_tensor = torch.load(first_val_tensor_path)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["39"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["len(first_val_tensor)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The first frame is not NaN or zeros.\n"]}],"source":["if torch.isnan(first_val_tensor[0]).any():\n","    print(\"The first frame contains NaN values!\")\n","elif torch.equal(first_val_tensor[0], torch.zeros_like(first_val_tensor[0])):\n","    print(\"The first frame is all zeros!\")\n","else:\n","    print(\"The first frame is not NaN or zeros.\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.status.busy":"2023-10-27T11:41:47.687336Z","iopub.status.idle":"2023-10-27T11:41:47.687783Z","shell.execute_reply":"2023-10-27T11:41:47.687581Z","shell.execute_reply.started":"2023-10-27T11:41:47.687560Z"},"trusted":true},"outputs":[],"source":["# torch.cuda.empty_cache()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
