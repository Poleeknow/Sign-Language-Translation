{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = self.dataframe['attachment_id'][idx]\n",
    "        image = torch.load(data_path)\n",
    "\n",
    "        # image = transforms.ToPILImage()(image)\n",
    "\n",
    "        # Ensure image is not a list; convert to tensor if it is a PIL Image\n",
    "        if isinstance(image, list):\n",
    "            raise ValueError(f\"Image at index {idx} was a list, not a tensor or PIL Image.\")\n",
    "        \n",
    "\n",
    "        label_mapping = {}  # Define a mapping from label names to indices\n",
    "        label = label_mapping.get(self.dataframe['text'][idx], -1) \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for compairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3DConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model3DConvNet, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3d = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.fc = nn.Linear(64*64*64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool3d(torch.relu(self.conv3d(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStream3DConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TwoStream3DConvNet, self).__init__()\n",
    "        self.spatial_stream = Model3DConvNet(num_classes)\n",
    "        self.temporal_stream = Model3DConvNet(num_classes)\n",
    "        \n",
    "    def forward(self, spatial_data, temporal_data):\n",
    "        spatial_out = self.spatial_stream(spatial_data)\n",
    "        temporal_out = self.temporal_stream(temporal_data)\n",
    "        out = (spatial_out + temporal_out) / 2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ModelResNet2D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModelResNet2D, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSwinTransformer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModelSwinTransformer, self).__init__()\n",
    "        self.swin_transformer = torch.hub.load('facebookresearch/swin-transformer', 'swin_base_patch4_window7_224_in22k', pretrained=True)\n",
    "        self.swin_transformer.head = nn.Linear(self.swin_transformer.head.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.swin_transformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMViTv2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModelMViTv2, self).__init__()\n",
    "        self.mvit_v2 = torch.hub.load('facebookresearch/mvit', 'mvit_base_16x4', pretrained=True)\n",
    "        self.mvit_v2.head = nn.Linear(self.mvit_v2.head.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mvit_v2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe = pd.read_csv(\"data/processed_annotations_train.csv\")\n",
    "val_dataframe = pd.read_csv(\"data/processed_annotations_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(dataframe=train_dataframe, transform=transform)\n",
    "val_dataset = CustomDataset(dataframe=val_dataframe, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = {\n",
    "    \"LSTM\": ModelLSTM(input_size=224*224*3, hidden_size=512, num_layers=2, num_classes=num_classes),\n",
    "    \"3D-ConvNet\": Model3DConvNet(num_classes=num_classes),\n",
    "    \"Two-Stream 3D-ConvNet\": TwoStream3DConvNet(num_classes=num_classes),\n",
    "    \"ResNet2D\": ModelResNet2D(num_classes=num_classes),\n",
    "    # \"Swin Transformer\": ModelSwinTransformer(num_classes=num_classes),\n",
    "    # \"MViTv2\": ModelMViTv2(num_classes=num_classes)\n",
    "}\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": train_data_loader,\n",
    "    \"val\": val_data_loader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {}\n",
    "num_epochs = 10\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in data_loaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loaders[\"train\"].dataset):.4f}')\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loaders['val']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_labels.append(labels.cpu())\n",
    "                all_preds.append(preds.cpu())\n",
    "\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        val_accuracy = torch.sum(all_preds == all_labels).item() / len(all_labels)\n",
    "        print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n",
    "\n",
    "    results[model_name] = val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization of Results\n",
    "model_names = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(model_names, accuracies, color='skyblue')\n",
    "plt.xlabel('Validation Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(models.keys())\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        model_1_name = model_names[i]\n",
    "        model_2_name = model_names[j]\n",
    "        \n",
    "        accuracies_model_1 = results[model_1_name]\n",
    "        accuracies_model_2 = results[model_2_name]\n",
    "        \n",
    "        t_stat, p_value = ttest_rel(accuracies_model_1, accuracies_model_2)\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(f\"{model_1_name} and {model_2_name} are significantly different (p={p_value:.3f}).\")\n",
    "        else:\n",
    "            print(f\"No significant difference between {model_1_name} and {model_2_name} (p={p_value:.3f}).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capsone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
